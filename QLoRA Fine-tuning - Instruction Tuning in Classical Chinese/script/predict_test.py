#!/usr/bin/env python3
"""
Prediction script without quantization for testing fixes
ä¸ä½¿ç”¨é‡åŒ–çš„é æ¸¬è…³æœ¬ï¼Œç”¨æ–¼æ¸¬è©¦ä¿®å¾©æ•ˆæœ
"""

import json
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import argparse
from utils import get_prompt, get_prompt_few_shot

# def get_prompt(instruction: str) -> str:
#     '''Format the instruction as a prompt for LLM.'''
#     return f"ä½ æ˜¯å¤æ–‡å°ˆå®¶ï¼Œè² è²¬æ–‡è¨€æ–‡èˆ‡ç™½è©±æ–‡çš„è½‰æ›ã€‚USER: {instruction} ASSISTANT:"

def remove_repetition(text, max_repeat=3):
    import re
    
    # ä¿®å¾©æ­£å‰‡è¡¨é”å¼ï¼ˆåŸæœ¬çš„\\1æ‡‰è©²æ˜¯\1ï¼‰
    text = re.sub(r'(.)\1{' + str(max_repeat-1) + ',}', r'\1', text)
    
    # ç§»é™¤é‡è¤‡çš„å¥å­æ¨¡å¼
    sentences = text.split('ã€‚')
    cleaned_sentences = []
    seen_sentences = set()
    
    for sentence in sentences:
        sentence = sentence.strip()
        if sentence and sentence not in seen_sentences:
            cleaned_sentences.append(sentence)
            seen_sentences.add(sentence)
    
    return 'ã€‚'.join(cleaned_sentences)

def clean_user_assistant_output(text):
    """Clean output to remove USER/ASSISTANT artifacts"""
    import re
    
    # Remove lines that start with USER: or ASSISTANT:
    lines = text.split('\n')
    cleaned_lines = []
    
    for line in lines:
        line = line.strip()
        # Skip lines that start with these patterns
        if line.startswith(('USER:', 'ASSISTANT:', 'user:', 'assistant:')):
            continue
        # Skip lines that only contain these patterns
        if line in ['USER:', 'ASSISTANT:', 'user:', 'assistant:']:
            continue
        if line:
            cleaned_lines.append(line)
    
    text = '\n'.join(cleaned_lines)
    
    # Remove common artifacts
    artifacts = [
        'USER:',
        'ASSISTANT:',
        'user:',
        'assistant:',
        'ç­”æ¡ˆï¼š',
        'å›ç­”ï¼š',
        'ç­”ï¼š'
    ]
    
    for artifact in artifacts:
        # Remove artifacts at the beginning
        if text.startswith(artifact):
            text = text[len(artifact):].strip()
        # Remove artifacts that appear after spaces
        text = re.sub(r'\s+' + re.escape(artifact), '', text)
    
    return text.strip()

def generate_response(model, tokenizer, instruction, max_new_tokens=128, temperature=0.7, top_p=0.9):
    """Generate response for a single instruction."""
    # Format prompt
    prompt = get_prompt(instruction)
    
    # Tokenize
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    # Generate
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            top_p=top_p,
            top_k=50,  # æ·»åŠ  top_k é™åˆ¶
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
            repetition_penalty=1.3,  # å¢åŠ é‡è¤‡æ‡²ç½°
            no_repeat_ngram_size=3,  # ç¦æ­¢3å­—é‡è¤‡
            # ç§»é™¤ early_stopping ä»¥é¿å…è­¦å‘Š
        )
    
    # Decode only the generated part (exclude the input prompt)
    generated_tokens = outputs[0][inputs["input_ids"].shape[1]:]
    response = tokenizer.decode(generated_tokens, skip_special_tokens=True)
    
    # Remove repetition and clean up
    response = remove_repetition(response)
    
    # Clean USER/ASSISTANT artifacts
    response = clean_user_assistant_output(response)
    
    # Stop at common ending patterns
    stop_patterns = [
        "\n\n",
        "USER:",
        "ASSISTANT:",
        "æŒ‡ä»¤ï¼š",
        "å›ç­”ï¼š",
        "ç¿»è­¯æˆå¤æ–‡ï¼š",
        "ç¿»è­¯æˆç™½è©±æ–‡ï¼š"
    ]
    
    for pattern in stop_patterns:
        if pattern in response:
            response = response.split(pattern)[0]
    
    return response.strip()

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_name", type=str, default="Qwen/Qwen2.5-3B", help="Base model name")
    parser.add_argument("--input_file", type=str, required=True, help="Input JSON file")
    parser.add_argument("--output_file", type=str, required=True, help="Output JSON file")
    parser.add_argument("--max_new_tokens", type=int, default=128, help="Maximum new tokens to generate")
    parser.add_argument("--temperature", type=float, default=0.7, help="Temperature for generation")
    parser.add_argument("--top_p", type=float, default=0.9, help="Top-p for generation")
    parser.add_argument("--max_samples", type=int, default=10, help="Maximum samples to process (for testing)")
    
    args = parser.parse_args()
    
    print("ğŸš€ é–‹å§‹æ¸¬è©¦ä¿®å¾©å¾Œçš„é æ¸¬åŠŸèƒ½...")
    print(f"æ¨¡å‹: {args.model_name}")
    print(f"è¼¸å…¥æ–‡ä»¶: {args.input_file}")
    print(f"è¼¸å‡ºæ–‡ä»¶: {args.output_file}")
    print(f"æœ€å¤§æ¨£æœ¬æ•¸: {args.max_samples}")
    
    # Load model and tokenizer (without quantization)
    print("ğŸ“¥ è¼‰å…¥æ¨¡å‹å’Œåˆ†è©å™¨...")
    
    model = AutoModelForCausalLM.from_pretrained(
        args.model_name,
        torch_dtype=torch.float16,  # ä½¿ç”¨ float16 è€Œéé‡åŒ–
        device_map="auto",
        trust_remote_code=True,
        use_cache=True
    )
    
    tokenizer = AutoTokenizer.from_pretrained(args.model_name, trust_remote_code=True)
    
    # Set pad token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
    
    model.eval()
    
    # Load input data
    print("ğŸ“Š è¼‰å…¥æ¸¬è©¦æ•¸æ“š...")
    with open(args.input_file, 'r', encoding='utf-8') as f:
        input_data = json.load(f)
    
    # é™åˆ¶æ¸¬è©¦æ¨£æœ¬æ•¸é‡
    if len(input_data) > args.max_samples:
        input_data = input_data[:args.max_samples]
        print(f"âš ï¸ é™åˆ¶æ¸¬è©¦æ¨£æœ¬ç‚º {args.max_samples} æ¢")
    
    # Generate responses
    print("ğŸ¯ é–‹å§‹ç”Ÿæˆå›ç­”...")
    results = []
    
    for i, item in enumerate(input_data):
        print(f"è™•ç† {i+1}/{len(input_data)}: {item['instruction'][:50]}...")
        
        instruction = item["instruction"]
        response = generate_response(
            model, tokenizer, instruction,
            max_new_tokens=args.max_new_tokens,
            temperature=args.temperature,
            top_p=args.top_p
        )
        
        print(f"   ç”Ÿæˆå›ç­”: {response[:50]}...")
        
        results.append({
            "id": item["id"],
            "output": response
        })
    
    # Save results
    print(f"ğŸ’¾ ä¿å­˜çµæœåˆ° {args.output_file}")
    with open(args.output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print("âœ… æ¸¬è©¦å®Œæˆï¼")
    
    # ç°¡å–®åˆ†æçµæœ
    print("\nğŸ“‹ çµæœåˆ†æ:")
    for i, result in enumerate(results[:3]):  # é¡¯ç¤ºå‰3å€‹çµæœ
        print(f"æ¨£æœ¬ {i+1}:")
        print(f"  å›ç­”: {result['output']}")
        print(f"  é•·åº¦: {len(result['output'])} å­—ç¬¦")
        
        # æª¢æŸ¥æ˜¯å¦æœ‰å•é¡Œæ¨¡å¼
        problems = ["USER:", "ASSISTANT:", "ç¿»è­¯æˆå¤æ–‡ï¼š", "æå¾·è£•ã€æç´³ã€æå¾·è£•"]
        found = [p for p in problems if p in result['output']]
        if found:
            print(f"  âš ï¸ ç™¼ç¾å•é¡Œ: {found}")
        else:
            print(f"  âœ… ç„¡æ˜é¡¯å•é¡Œ")
        print()

if __name__ == "__main__":
    main()