#!/usr/bin/env python3
"""
å„ªåŒ–çš„ QLoRA Fine-tuning è…³æœ¬
ç›®æ¨™ï¼šå°‡ public perplexity é™åˆ° 7.2 ä»¥ä¸‹
"""

import json
import torch
import os
from datasets import Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from utils import get_prompt, get_bnb_config
import argparse


def format_example_optimized(example):
    """å„ªåŒ–çš„æ¨£æœ¬æ ¼å¼åŒ–å‡½æ•¸"""
    instruction = example["instruction"]
    output = example["output"]
    
    # ä½¿ç”¨çš„ prompt æ ¼å¼
    # prompt = f"ä½ æ˜¯å¤æ–‡å°ˆå®¶ã€‚è«‹æ ¹æ“šä»¥ä¸‹æŒ‡ä»¤å®Œæˆä»»å‹™ã€‚\n\næŒ‡ä»¤ï¼š{instruction}\nå›ç­”ï¼š"
    prompt = get_prompt(instruction)

    # çµ„åˆ prompt å’Œè¼¸å‡º
    text = prompt + output
    
    return {"text": text}


def preprocess_function(examples, tokenizer, max_length=256):
    """é è™•ç†å‡½æ•¸"""
    # ä¿®å¾©: ç•¶ batched=True æ™‚ï¼Œexamples æ˜¯å­—å…¸ï¼ŒåŒ…å«åˆ—è¡¨
    texts = examples["text"]  # ç›´æ¥ç²å– text åˆ—è¡¨
    
    # Tokenize
    tokenized = tokenizer(
        texts,
        truncation=True,
        padding=False,
        max_length=max_length,
        return_tensors=None,
    )
    
    # è¨­ç½® labelsï¼ˆç”¨æ–¼è¨ˆç®— lossï¼‰
    tokenized["labels"] = tokenized["input_ids"].copy()
    
    return tokenized


def load_and_prepare_data(train_file, tokenizer, max_length=512):
    """è¼‰å…¥å’Œæº–å‚™è¨“ç·´æ•¸æ“š"""
    print(f"ğŸ“š è¼‰å…¥è¨“ç·´æ•¸æ“š: {train_file}")
    
    with open(train_file, 'r', encoding='utf-8') as f:
        train_data = json.load(f)
    
    print(f"âœ… è¼‰å…¥ {len(train_data)} ç­†è¨“ç·´æ•¸æ“š")
    
    # æ ¼å¼åŒ–æ•¸æ“š
    formatted_data = [format_example_optimized(ex) for ex in train_data]
    
    # å‰µå»º Dataset
    dataset = Dataset.from_list(formatted_data)
    
    # é è™•ç†
    tokenized_dataset = dataset.map(
        lambda examples: preprocess_function(examples, tokenizer, max_length),
        batched=True,
        remove_columns=dataset.column_names,
    )
    
    print(f"ğŸ”„ é è™•ç†å®Œæˆï¼Œå…± {len(tokenized_dataset)} ç­†æ•¸æ“š")
    
    return tokenized_dataset


def get_optimized_lora_config():
    """ç²å–å„ªåŒ–çš„ LoRA é…ç½®"""
    return LoraConfig(
        task_type="CAUSAL_LM",
        inference_mode=False,
        r=32,  # å¢åŠ  rank
        lora_alpha=64,  # å¢åŠ  alpha
        lora_dropout=0.05,  # é™ä½ dropout
        target_modules=[
            "q_proj", "k_proj", "v_proj", "o_proj",
            "gate_proj", "up_proj", "down_proj"
        ],
        bias="none",
    )


def get_optimized_training_args(output_dir, num_train_epochs=5):
    """ç²å–å„ªåŒ–çš„è¨“ç·´åƒæ•¸"""
    return TrainingArguments(
        output_dir=output_dir,
        
        # åŸºæœ¬è¨­ç½®
        num_train_epochs=num_train_epochs,
        per_device_train_batch_size=2,  # å¢åŠ  batch size
        gradient_accumulation_steps=8,  # ä½¿ç”¨æ¢¯åº¦ç´¯ç©
        
        # å­¸ç¿’ç‡è¨­ç½®
        learning_rate=4e-4,  # æé«˜å­¸ç¿’ç‡
        lr_scheduler_type="cosine",  # ä½¿ç”¨ cosine scheduler
        warmup_ratio=0.1,  # æ·»åŠ  warmup
        
        # å„ªåŒ–å™¨è¨­ç½®
        optim="adamw_torch",
        weight_decay=0.01,
        adam_beta2=0.999,
        
        # ä¿å­˜å’Œè©•ä¼°
        save_strategy="epoch",
        save_total_limit=3,
        logging_steps=50,
        
        # å…¶ä»–è¨­ç½®
        dataloader_drop_last=True,
        remove_unused_columns=False,
        group_by_length=True,  # æŒ‰é•·åº¦åˆ†çµ„æé«˜æ•ˆç‡
        
        # æ··åˆç²¾åº¦
        fp16=True,
        
        # é˜²æ­¢éæ“¬åˆ
        save_safetensors=True,
        
        # å ±å‘Š
        report_to=None,  # ç¦ç”¨ wandb
    )


def train_optimized_model(
    base_model_name="Qwen/Qwen3-4B",  # ä½¿ç”¨æ›´ç©©å®šçš„ç‰ˆæœ¬
    train_file="data/train.json",
    output_dir="./optimized_adapter_checkpoint",
    max_length=512,
    num_epochs=5
):
    """è¨“ç·´å„ªåŒ–çš„æ¨¡å‹"""
    
    print("ğŸš€ é–‹å§‹å„ªåŒ–çš„ QLoRA Fine-tuning")
    print("=" * 60)
    
    # è¼‰å…¥ tokenizer
    print(f"ğŸ“¥ è¼‰å…¥ tokenizer: {base_model_name}")
    tokenizer = AutoTokenizer.from_pretrained(base_model_name)
    
    # è¨­ç½® pad token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # è¼‰å…¥æ¨¡å‹
    print(f"ğŸ“¥ è¼‰å…¥æ¨¡å‹: {base_model_name}")
    bnb_config = get_bnb_config()
    
    model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
    )
    
    # æº–å‚™æ¨¡å‹ç”¨æ–¼è¨“ç·´
    model = prepare_model_for_kbit_training(model)
    
    # æ‡‰ç”¨ LoRA
    lora_config = get_optimized_lora_config()
    model = get_peft_model(model, lora_config)
    
    print(f"ğŸ”§ LoRA é…ç½®:")
    print(f"   - Rank (r): {lora_config.r}")
    print(f"   - Alpha: {lora_config.lora_alpha}")
    print(f"   - Dropout: {lora_config.lora_dropout}")
    print(f"   - Target modules: {lora_config.target_modules}")
    
    # è¼‰å…¥å’Œæº–å‚™æ•¸æ“š
    train_dataset = load_and_prepare_data(train_file, tokenizer, max_length)
    
    # è¨­ç½®è¨“ç·´åƒæ•¸
    training_args = get_optimized_training_args(output_dir, num_epochs)
    
    print(f"ğŸ¯ è¨“ç·´é…ç½®:")
    print(f"   - Epochs: {num_epochs}")
    print(f"   - Batch size: {training_args.per_device_train_batch_size}")
    print(f"   - Gradient accumulation: {training_args.gradient_accumulation_steps}")
    print(f"   - Learning rate: {training_args.learning_rate}")
    print(f"   - Scheduler: {training_args.lr_scheduler_type}")
    
    # æ•¸æ“šæ”¶é›†å™¨
    data_collator = DataCollatorForSeq2Seq(
        tokenizer=tokenizer,
        model=model,
        padding=True,
    )
    
    # å‰µå»º Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        data_collator=data_collator,
    )
    
    # é–‹å§‹è¨“ç·´
    print(f"\nğŸ‹ï¸ é–‹å§‹è¨“ç·´...")
    trainer.train()
    
    # ä¿å­˜æ¨¡å‹
    print(f"ğŸ’¾ ä¿å­˜æ¨¡å‹åˆ°: {output_dir}")
    trainer.save_model()
    tokenizer.save_pretrained(output_dir)
    
    print(f"âœ… è¨“ç·´å®Œæˆï¼")
    print(f"ğŸ“ æ¨¡å‹ä¿å­˜ä½ç½®: {output_dir}")
    
    return output_dir


def main():
    parser = argparse.ArgumentParser(description="å„ªåŒ–çš„ QLoRA Fine-tuning")
    parser.add_argument("--base_model", default="Qwen/Qwen3-4B", help="åŸºç¤æ¨¡å‹åç¨±")
    parser.add_argument("--train_file", default="data/train.json", help="è¨“ç·´æ•¸æ“šæ–‡ä»¶")
    parser.add_argument("--output_dir", default="./optimized_adapter_checkpoint", help="è¼¸å‡ºç›®éŒ„")
    parser.add_argument("--max_length", type=int, default=256, help="æœ€å¤§åºåˆ—é•·åº¦")
    parser.add_argument("--epochs", type=int, default=3, help="è¨“ç·´è¼ªæ•¸")
    
    args = parser.parse_args()
    
    # æª¢æŸ¥è¨“ç·´æ•¸æ“šæ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.train_file):
        print(f"âŒ æ‰¾ä¸åˆ°è¨“ç·´æ•¸æ“š: {args.train_file}")
        return
    
    # é–‹å§‹è¨“ç·´
    try:
        output_dir = train_optimized_model(
            base_model_name=args.base_model,
            train_file=args.train_file,
            output_dir=args.output_dir,
            max_length=args.max_length,
            num_epochs=args.epochs
        )
        
        print(f"\nğŸ¯ ä¸‹ä¸€æ­¥:")
        print(f"1. æ¸¬è©¦æ–°æ¨¡å‹çš„ perplexity:")
        print(f"   python ppl.py --base_model_path {args.base_model} --peft_path {output_dir} --test_data_path data/public_test.json")
        
        print(f"2. å¦‚æœ perplexity ä»ç„¶å¾ˆé«˜ï¼Œå¯ä»¥:")
        print(f"   - å¢åŠ  epochs: --epochs 8")
        print(f"   - èª¿æ•´å­¸ç¿’ç‡")
        print(f"   - ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹")
        
    except Exception as e:
        print(f"âŒ è¨“ç·´å¤±æ•—: {e}")
        print(f"ğŸ’¡ å¯èƒ½çš„è§£æ±ºæ–¹æ¡ˆ:")
        print(f"   1. æª¢æŸ¥ GPU è¨˜æ†¶é«”æ˜¯å¦è¶³å¤ ")
        print(f"   2. æ¸›å°‘ batch_size")
        print(f"   3. æ¸›å°‘ max_length")


if __name__ == "__main__":
    main()