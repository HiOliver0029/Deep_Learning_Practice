#!/usr/bin/env python3
"""
è¨˜æ†¶é«”å„ªåŒ–ç‰ˆ QLoRA Fine-tuning è…³æœ¬ï¼ˆå·²ä¿®æ­£ç‰ˆï¼šæ”¹è‰¯ preprocess èˆ‡ data collatorï¼Œlabels ä¸­ prompt token è¨­ç‚º -100ï¼‰
"""

import json
import torch
import os
import gc
from datasets import Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from utils import get_prompt, get_prompt_few_shot, get_bnb_config
import argparse
from typing import List, Dict, Callable, Any


def setup_memory_optimization():
    """è¨­ç½®è¨˜æ†¶é«”å„ªåŒ–"""
    print("ğŸ”§ è¨­ç½®è¨˜æ†¶é«”å„ªåŒ–.")
    
    # æ¸…ç†ç¾æœ‰è¨˜æ†¶é«”
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gc.collect()
    
    # è¨­ç½®ç’°å¢ƒè®Šæ•¸
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
    
    # è¨­ç½® PyTorch å¾Œç«¯
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True
    
    print("âœ… è¨˜æ†¶é«”å„ªåŒ–è¨­ç½®å®Œæˆ")


def format_example_optimized(example):
    """å„ªåŒ–çš„æ¨£æœ¬æ ¼å¼åŒ–å‡½æ•¸"""
    instruction = example["instruction"]
    output = example["output"]
    
    # ä½¿ç”¨å„ªåŒ–çš„ prompt æ ¼å¼
    prompt = get_prompt(instruction)
    
    # çµ„åˆ prompt å’Œè¼¸å‡ºï¼ˆåƒ…ç”¨æ–¼å„²å­˜åŸå§‹æ–‡å­—ï¼‰
    text = prompt + output
    
    return {"text": text, "instruction": instruction, "output": output}


# -----------------------------
# æ–°ç‰ˆ preprocess: æœƒåˆ†é–‹ tokenize prompt èˆ‡ outputï¼Œlabels ä¸­ prompt è¨­ç‚º -100
# -----------------------------
def preprocess_function_memory_efficient(
    examples: Dict[str, List[str]],
    tokenizer,
    max_length: int = 512,
    get_prompt_fn: Callable[[str], str] = None,
    instruction_key: str = "instruction",
    output_key: str = "output",
):
    """
    å°‡ examplesï¼ˆå« instruction èˆ‡ output æ¬„ä½ï¼‰è½‰ç‚º input_ids, attention_mask, labelsã€‚
    labels ä¸­ prompt tokens è¨­ç‚º -100ï¼Œä½¿ loss åƒ…è¨ˆç®— output éƒ¨åˆ†ã€‚
    è‹¥ prompt+output é•·åº¦è¶…é max_lengthï¼Œæœƒå„ªå…ˆä¿ç•™ outputï¼Œå¾ prompt å·¦å´æˆªæ–·ã€‚
    è¿”å›å€¼ç‚º dict of listsï¼š{'input_ids': [...], 'attention_mask': [...], 'labels': [...]}ï¼Œé©åˆ HF Dataset.map(batched=True)
    """
    if get_prompt_fn is None:
        get_prompt_fn = get_prompt

    batch_input_ids = []
    batch_attention_mask = []
    batch_labels = []

    instrs = examples.get(instruction_key, [])
    outs = examples.get(output_key, [])

    for instruction, output in zip(instrs, outs):
        prompt = get_prompt_fn(instruction)

        # åˆ†åˆ¥ tokenize prompt èˆ‡ outputï¼Œä¸åŠ å…¥ special tokensï¼ˆç”±æˆ‘å€‘æ§åˆ¶ï¼‰
        prompt_ids = tokenizer(prompt, add_special_tokens=False)["input_ids"]
        output_ids = tokenizer(output, add_special_tokens=False)["input_ids"]

        # åœ¨ output çµå°¾åŠ  eosï¼ˆè‹¥æœ‰è¨­å®šï¼‰
        if tokenizer.eos_token_id is not None:
            output_ids = output_ids + [tokenizer.eos_token_id]

        # ä¿è­‰ä¸è¶…é max_lengthï¼šå„ªå…ˆä¿ç•™ output
        total_len = len(prompt_ids) + len(output_ids)
        if total_len > max_length:
            overflow = total_len - max_length
            # è‹¥éœ€è¦åˆªé™¤ï¼Œå…ˆå¾ prompt å·¦å´åˆª
            if overflow >= len(prompt_ids):
                # prompt æœƒè¢«å…¨éƒ¨ç§»é™¤ï¼Œå‰©ä¸‹é‚„éœ€åˆªé™¤ overflow - len(prompt_ids) tokens å¾ output å·¦å´
                prompt_ids = []
                rem = overflow - len(prompt_ids)
                if rem >= len(output_ids):
                    # degenerate: output æ¯” max_length é‚„é•· -> æˆªå– output çš„å°¾éƒ¨
                    output_ids = output_ids[-max_length:]
                else:
                    output_ids = output_ids[rem:]
            else:
                # ä¸€èˆ¬æƒ…æ³ï¼šå¾ prompt é–‹é ­åˆªé™¤ overflow å€‹ token
                prompt_ids = prompt_ids[overflow:]

        input_ids = prompt_ids + output_ids
        attention_mask = [1] * len(input_ids)

        # labels: prompt -> -100, output -> token ids
        labels = [-100] * len(prompt_ids) + output_ids.copy()

        assert len(input_ids) == len(attention_mask) == len(labels)

        batch_input_ids.append(input_ids)
        batch_attention_mask.append(attention_mask)
        batch_labels.append(labels)

    return {
        "input_ids": batch_input_ids,
        "attention_mask": batch_attention_mask,
        "labels": batch_labels,
    }


# -----------------------------
# Data collator for causal LM: pads input_ids & attention_mask, pads labels with -100
# -----------------------------
class DataCollatorForCausalLMWithPad:
    """
    Pads a batch of dicts with keys: input_ids (list[int]), attention_mask (list[int]), labels (list[int]).
    Uses tokenizer.pad to pad input_ids & attention_mask and pads labels with -100.
    Returns PyTorch tensors.
    """
    def __init__(self, tokenizer, pad_to_multiple_of: int = None):
        self.tokenizer = tokenizer
        self.pad_to_multiple_of = pad_to_multiple_of

        if self.tokenizer.pad_token_id is None:
            # è‹¥ tokenizer æ²’ pad_tokenï¼Œä½¿ç”¨ eos ä½œç‚º pad
            if self.tokenizer.eos_token is not None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            else:
                self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})

    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
        # æº–å‚™è¦çµ¦ tokenizer.pad çš„çµæ§‹
        inputs = [{"input_ids": f["input_ids"], "attention_mask": f["attention_mask"]} for f in features]

        batch = self.tokenizer.pad(
            inputs,
            padding=True,
            return_tensors="pt",
            pad_to_multiple_of=self.pad_to_multiple_of,
        )

        # labels pad
        max_len = batch["input_ids"].shape[1]
        labels = []
        for f in features:
            lab = f.get("labels", None)
            if lab is None:
                labels.append([-100] * max_len)
            else:
                lab_len = len(lab)
                if lab_len < max_len:
                    lab_padded = lab + [-100] * (max_len - lab_len)
                else:
                    lab_padded = lab[:max_len]
                labels.append(lab_padded)

        batch["labels"] = torch.tensor(labels, dtype=torch.long)

        return batch


def get_memory_efficient_lora_config():
    """ç²å–è¨˜æ†¶é«”é«˜æ•ˆçš„ LoRA é…ç½®"""
    return LoraConfig(
        task_type="CAUSAL_LM",
        inference_mode=False,
        r=64,  # æ¸›å°‘ rank ä»¥ç¯€çœè¨˜æ†¶é«”
        lora_alpha=64,  # ç›¸æ‡‰æ¸›å°‘ alpha
        lora_dropout=0.1,
        target_modules=[
            "q_proj", "v_proj",  # åªé‡å°é—œéµæ¨¡çµ„ï¼Œæ¸›å°‘åƒæ•¸é‡
            "o_proj", "down_proj"
        ],
        bias="none",
    )


def get_memory_efficient_training_args(output_dir, num_train_epochs=6):
    """ç²å–è¨˜æ†¶é«”é«˜æ•ˆçš„è¨“ç·´åƒæ•¸"""
    return TrainingArguments(
        output_dir=output_dir,
        
        # åŸºæœ¬è¨­ç½®
        num_train_epochs=num_train_epochs,
        per_device_train_batch_size=2,  # æœ€å° batch size
        gradient_accumulation_steps=8,  # å¢åŠ æ¢¯åº¦ç´¯ç©è£œå„Ÿ
        
        # å­¸ç¿’ç‡è¨­ç½®
        learning_rate=2e-4,  # ç¨å¾®é™ä½å­¸ç¿’ç‡
        lr_scheduler_type="linear",
        warmup_ratio=0.1,
        
        # å„ªåŒ–å™¨è¨­ç½®
        optim="adamw_torch",
        weight_decay=0.01,
        
        # ä¿å­˜å’Œè©•ä¼°
        save_strategy="epoch",
        save_total_limit=6,  # æ¸›å°‘ä¿å­˜çš„æ¨¡å‹æ•¸é‡
        logging_steps=50,
        
        # è¨˜æ†¶é«”å„ªåŒ–
        dataloader_drop_last=True,
        remove_unused_columns=False,
        group_by_length=True,
        
        # æ··åˆç²¾åº¦å’Œè¨˜æ†¶é«”å„ªåŒ–
        fp16=True,
        dataloader_pin_memory=False,  # é—œé–‰ pin memory
        gradient_checkpointing=True,  # é–‹å•Ÿæ¢¯åº¦æª¢æŸ¥é»
        
        # å…¶ä»–å„ªåŒ–
        save_safetensors=True,
        report_to=None,
    )


def load_and_prepare_data_efficient(train_file, tokenizer, max_length=512):
    """è¨˜æ†¶é«”é«˜æ•ˆçš„æ•¸æ“šè¼‰å…¥"""
    print(f"ğŸ“š è¼‰å…¥è¨“ç·´æ•¸æ“š: {train_file}")
    
    with open(train_file, 'r', encoding='utf-8') as f:
        train_data = json.load(f)
    
    print(f"âœ… è¼‰å…¥ {len(train_data)} ç­†è¨“ç·´æ•¸æ“š")
    
    # æ ¼å¼åŒ–æ•¸æ“šï¼ˆä¿ç•™ instruction èˆ‡ output æ¬„ä½ä»¥ä¾›æ–°ç‰ˆ preprocess ä½¿ç”¨ï¼‰
    formatted_data = [format_example_optimized(ex) for ex in train_data]
    
    # å‰µå»º Dataset
    dataset = Dataset.from_list(formatted_data)
    
    # åˆ†æ‰¹é è™•ç†ä»¥ç¯€çœè¨˜æ†¶é«”
    tokenized_dataset = dataset.map(
        lambda examples: preprocess_function_memory_efficient(
            examples, tokenizer, max_length, get_prompt_fn=get_prompt, instruction_key="instruction", output_key="output"
        ),
        batched=True,
        batch_size=100,  # å°æ‰¹æ¬¡è™•ç†
        remove_columns=dataset.column_names,
    )
    
    print(f"ğŸ”„ é è™•ç†å®Œæˆï¼Œå…± {len(tokenized_dataset)} ç­†æ•¸æ“š")
    
    return tokenized_dataset


def train_memory_efficient_model(
    base_model_name="Qwen/Qwen3-4B",
    train_file="data/train.json",
    output_dir="./memory_efficient_adapter",
    max_length=512,
    num_epochs=6
):
    """è¨˜æ†¶é«”é«˜æ•ˆçš„æ¨¡å‹è¨“ç·´"""
    
    print("ğŸš€ é–‹å§‹è¨˜æ†¶é«”å„ªåŒ– QLoRA Fine-tuning")
    print("=" * 60)
    
    # è¨­ç½®è¨˜æ†¶é«”å„ªåŒ–
    setup_memory_optimization()
    
    # è¼‰å…¥ tokenizer
    print(f"ğŸ“¥ è¼‰å…¥ tokenizer: {base_model_name}")
    tokenizer = AutoTokenizer.from_pretrained(base_model_name)
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # è¼‰å…¥æ¨¡å‹ï¼ˆä½¿ç”¨æ›´ç©æ¥µçš„é‡åŒ–ï¼‰
    print(f"ğŸ“¥ è¼‰å…¥æ¨¡å‹: {base_model_name}")
    bnb_config = get_bnb_config()
    
    model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
        torch_dtype=torch.float16,  # å¼·åˆ¶ä½¿ç”¨ fp16
        low_cpu_mem_usage=True,     # é™ä½ CPU è¨˜æ†¶é«”ä½¿ç”¨
    )
    
    # æº–å‚™æ¨¡å‹ç”¨æ–¼è¨“ç·´
    model = prepare_model_for_kbit_training(model)
    
    # æ‡‰ç”¨è¨˜æ†¶é«”é«˜æ•ˆçš„ LoRA
    lora_config = get_memory_efficient_lora_config()
    model = get_peft_model(model, lora_config)
    
    print(f"ğŸ”§ è¨˜æ†¶é«”å„ªåŒ– LoRA é…ç½®:")
    print(f"   - Rank (r): {lora_config.r}")
    print(f"   - Alpha: {lora_config.lora_alpha}")
    print(f"   - Target modules: {lora_config.target_modules}")
    
    # é¡¯ç¤ºæ¨¡å‹åƒæ•¸é‡
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    print(f"ğŸ¯ å¯è¨“ç·´åƒæ•¸: {trainable_params:,} / {total_params:,} ({100 * trainable_params / total_params:.2f}%)")
    
    # è¼‰å…¥å’Œæº–å‚™æ•¸æ“š
    train_dataset = load_and_prepare_data_efficient(train_file, tokenizer, max_length)
    
    # è¨­ç½®è¨“ç·´åƒæ•¸
    training_args = get_memory_efficient_training_args(output_dir, num_epochs)
    
    print(f"ğŸ¯ è¨˜æ†¶é«”å„ªåŒ–è¨“ç·´é…ç½®:")
    print(f"   - Epochs: {num_epochs}")
    print(f"   - Batch size: {training_args.per_device_train_batch_size}")
    print(f"   - Gradient accumulation: {training_args.gradient_accumulation_steps}")
    print(f"   - Max length: {max_length}")
    print(f"   - Gradient checkpointing: {training_args.gradient_checkpointing}")
    
    # ä½¿ç”¨è‡ªè£½çš„ causal-lm data collatorï¼ˆæœƒæŠŠ labels è£œ -100ï¼‰
    data_collator = DataCollatorForCausalLMWithPad(tokenizer)
    
    # å‰µå»º Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        data_collator=data_collator,
    )
    
    # æ¸…ç†è¨˜æ†¶é«”
    torch.cuda.empty_cache()
    gc.collect()
    
    # é–‹å§‹è¨“ç·´
    print(f"\nğŸ‹ï¸ é–‹å§‹è¨˜æ†¶é«”å„ªåŒ–è¨“ç·´.")
    print(f"ğŸ’¾ é ä¼° GPU è¨˜æ†¶é«”éœ€æ±‚: ~3-4 GB")
    
    try:
        trainer.train()
        
        # ä¿å­˜æ¨¡å‹
        print(f"ğŸ’¾ ä¿å­˜æ¨¡å‹åˆ°: {output_dir}")
        trainer.save_model()
        tokenizer.save_pretrained(output_dir)
        
        print(f"âœ… è¨“ç·´å®Œæˆï¼")
        
    except RuntimeError as e:
        if "out of memory" in str(e):
            print(f"âŒ è¨˜æ†¶é«”ä»ç„¶ä¸è¶³ï¼")
            print(f"ğŸ’¡ è«‹å˜—è©¦ä»¥ä¸‹è§£æ±ºæ–¹æ¡ˆ:")
            print(f"   1. é€²ä¸€æ­¥æ¸›å°‘ batch_size åˆ° 1")
            print(f"   2. æ¸›å°‘ max_length åˆ° 128")
            print(f"   3. æ¸›å°‘ LoRA rank åˆ° 8")
            print(f"   4. æ‰‹å‹•çµ‚æ­¢å…¶ä»– GPU é€²ç¨‹")
            raise e
        else:
            raise e
    
    return output_dir


def main():
    parser = argparse.ArgumentParser(description="è¨˜æ†¶é«”å„ªåŒ– QLoRA Fine-tuning")
    parser.add_argument("--base_model", default="Qwen/Qwen3-4B", help="åŸºç¤æ¨¡å‹åç¨±")
    parser.add_argument("--train_file", default="data/train.json", help="è¨“ç·´æ•¸æ“šæ–‡ä»¶")
    parser.add_argument("--output_dir", default="./memory_efficient_adapter", help="è¼¸å‡ºç›®éŒ„")
    parser.add_argument("--max_length", type=int, default=512, help="æœ€å¤§åºåˆ—é•·åº¦")
    parser.add_argument("--epochs", type=int, default=6, help="è¨“ç·´è¼ªæ•¸")
    
    args = parser.parse_args()
    
    # æª¢æŸ¥ GPU è¨˜æ†¶é«”
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"ğŸ” GPU ç¸½è¨˜æ†¶é«”: {gpu_memory:.2f} GB")
        
        if gpu_memory < 8:
            print(f"âš ï¸ GPU è¨˜æ†¶é«”è¼ƒå°‘ï¼Œå°‡ä½¿ç”¨è¶…ä¿å®ˆè¨­ç½®")
            args.max_length = 128
    
    # é–‹å§‹è¨“ç·´
    try:
        output_dir = train_memory_efficient_model(
            base_model_name=args.base_model,
            train_file=args.train_file,
            output_dir=args.output_dir,
            max_length=args.max_length,
            num_epochs=args.epochs
        )
        
        print(f"\nğŸ¯ è¨“ç·´å®Œæˆï¼ä¸‹ä¸€æ­¥:")
        print(f"1. æ¸¬è©¦æ–°æ¨¡å‹çš„ perplexity:")
        print(f"   python ppl.py --base_model_path {args.base_model} --peft_path {output_dir} --test_data_path data/public_test.json")
        
    except Exception as e:
        print(f"âŒ è¨“ç·´å¤±æ•—: {e}")


if __name__ == "__main__":
    main()
